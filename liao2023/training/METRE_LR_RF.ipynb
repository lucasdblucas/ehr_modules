{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tBIxBus3XeQn"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%cd /content/drive/My Drive/ColabNotebooks/MIMIC/MEEP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5vdc0kiOlKR_"
      },
      "outputs": [],
      "source": [
        "pip install bayesian-optimization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HDyvFB4Rl4Xv"
      },
      "outputs": [],
      "source": [
        "import random \n",
        "import torch.nn as nn \n",
        "import numpy as np \n",
        "import pandas as pd \n",
        "import scipy.stats as st\n",
        "import argparse\n",
        "import sys\n",
        "import os\n",
        "import json\n",
        "import pickle \n",
        "from tqdm import tqdm\n",
        "from bayes_opt import BayesianOptimization\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.metrics import average_precision_score\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import scipy.stats as ss\n",
        "import rocutils.roc_utils as rr\n",
        "from rocutils.roc_utils import *\n",
        "\n",
        "# Date\n",
        "from datetime import date\n",
        "today = date.today()\n",
        "date = today.strftime(\"%m%d\")\n",
        "\n",
        "# plotting \n",
        "import matplotlib \n",
        "import matplotlib.pyplot as plt\n",
        "from palettable.cmocean.sequential import Dense_20\n",
        "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
        "plt.style.use(\"bmh\")\n",
        "matplotlib.rcParams[\"figure.dpi\"] = 300\n",
        "plt.rcParams[\"font.weight\"] = \"bold\"\n",
        "plt.rcParams[\"axes.labelweight\"] = \"bold\"\n",
        "legend_properties = {'weight':'bold', 'size': 10}\n",
        "f_sm = nn.Softmax(dim=1)\n",
        "kf = KFold(n_splits=10, random_state=42, shuffle=True)\n",
        "\n",
        "# make dir and write json files \n",
        "def write_json(target_path, target_file, data):\n",
        "    if not os.path.exists(target_path):\n",
        "        try:\n",
        "            os.makedirs(target_path)\n",
        "        except Exception as e:\n",
        "            print(e)\n",
        "            raise\n",
        "    with open(os.path.join(target_path, target_file), 'w') as f:\n",
        "        json.dump(data, f)\n",
        "\n",
        "# count model trainable params \n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "# combine train and val and based on the cross validation index, redistribute train and val \n",
        "def get_cv_data(train_data, dev_data, train_target, dev_target, train_index, dev_index):\n",
        "    trainval_head = train_data + dev_data\n",
        "    trainval_static = np.concatenate((train_target, dev_target), axis=0)\n",
        "    train_cv = [trainval_head[i] for i in train_index]\n",
        "    train_cvl = [trainval_static[i] for i in train_index]\n",
        "    dev_cv = [trainval_head[i] for i in dev_index]\n",
        "    dev_cvl = [trainval_static[i] for i in dev_index]\n",
        "    return train_cv, dev_cv, np.asarray(train_cvl), np.asarray(dev_cvl)\n",
        "\n",
        "# calcuate accuracy \n",
        "def cal_acc(pred, label):\n",
        "    pred_t = torch.concat(pred)\n",
        "    prediction =  torch.argmax(pred_t, dim=-1).unsqueeze(-1)\n",
        "    label_t = torch.concat(label)\n",
        "    acc = (prediction == label_t).sum()/len(pred_t)\n",
        "    return acc\n",
        "\n",
        "# calcualte accuracy for positive classes \n",
        "def cal_pos_acc(pred, label, pos_ind):\n",
        "    pred_t = torch.concat(pred)\n",
        "    prediction =  torch.argmax(pred_t, dim=-1).unsqueeze(-1)\n",
        "    label_t = torch.concat(label)\n",
        "    # positive index\n",
        "    ind = [i for i in range(len(pred_t)) if label_t[i] == pos_ind]\n",
        "    acc = (prediction[ind] == label_t[ind]).sum()/len(ind)\n",
        "    return acc\n",
        "\n",
        "# random sampling \n",
        "class DictDist():\n",
        "    def __init__(self, dict_of_rvs): self.dict_of_rvs = dict_of_rvs\n",
        "    def rvs(self, n):\n",
        "        a = {k: v.rvs(n) for k, v in self.dict_of_rvs.items()}\n",
        "        out = []\n",
        "        for i in range(n): out.append({k: vs[i] for k, vs in a.items()})\n",
        "        return out\n",
        "\n",
        "# filter ICU stays based on LOS needed: 48+6, 4+6, 12+6\n",
        "def filter_los(static_data, vitals_data, thresh, gap):\n",
        "    # (200, 80)\n",
        "    los = [i.shape[1] for i in vitals_data]\n",
        "    ind = [i for i in range(len(los)) if los[i]>=(thresh+gap) and np.isnan(static_data[i, 0]) == False]\n",
        "    vitals_reduce = [vitals_data[i][:, :thresh] for i in ind] \n",
        "    static_data = static_data[ind]\n",
        "    return static_data, vitals_reduce\n",
        "\n",
        "# filter for the ARF tasks \n",
        "def filter_arf(args, vital):\n",
        "    vital_reduce = []\n",
        "    target = []\n",
        "    for i in range(len(vital)):\n",
        "        arf_flag = np.where(vital[i][184, :] == 1)[0]\n",
        "        peep_flag = np.union1d(np.where(vital[i][157, :] == 1)[0], np.where(vital[i][159, :] == 1)[0])\n",
        "        if len(arf_flag) == 0:\n",
        "            if len(peep_flag) > 0:\n",
        "                if peep_flag[0] >= (args.thresh + args.gap):\n",
        "                    vital_reduce.append(vital[i][:, :args.thresh])\n",
        "                    target.append(1)\n",
        "            else:\n",
        "                vital_reduce.append(vital[i][:, :args.thresh])\n",
        "                target.append(0)\n",
        "        elif len(arf_flag) > 0:\n",
        "            if arf_flag[0] >= (args.thresh + args.gap):\n",
        "                if (len(peep_flag) > 0 and peep_flag[0] >= (args.thresh + args.gap)) or len(peep_flag) == 0:\n",
        "                    vital_reduce.append(vital[i][:, :args.thresh])\n",
        "                    target.append(1)\n",
        "    return vital_reduce, np.asarray(target)\n",
        "\n",
        "# filter for the shock task \n",
        "def filter_shock(args, vital):\n",
        "    vital_reduce = []\n",
        "    target = []\n",
        "    for i in range(len(vital)):\n",
        "        shock_flag = np.where(vital[i][186:191].sum(axis=0) >= 1)[0]\n",
        "        if len(shock_flag) == 0:\n",
        "            vital_reduce.append(vital[i][:, :args.thresh])\n",
        "            target.append(0)\n",
        "        elif len(shock_flag) > 0:\n",
        "            if shock_flag[0] >= (args.thresh + args.gap):\n",
        "                vital_reduce.append(vital[i][:, :args.thresh])\n",
        "                target.append(1)\n",
        "    return vital_reduce, np.asarray(target)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# set data path for both MIMIC and eICU \n",
        "datapath = '/content/drive/MyDrive/ColabNotebooks/MIMIC/Extract/MEEP/Extracted_Feb_2023/MIMIC_compile_0218_2023_2.npy'\n",
        "datapath_cv = '/content/drive/MyDrive/ColabNotebooks/MIMIC/Extract/MEEP/Extracted_Feb_2023/MIMIC_compile_0218_2023_2.npy'\n",
        "# Target 0: hospital mortality; 1: ARF; 2: Shock \n",
        "# Threshold list by hour: 48, 4, 12\n",
        "# Gap hour: 6 \n",
        "# Models : LR, RF\n",
        "target_list = [0, 1, 1, 2, 2]\n",
        "thresh_list = [24, 2, 6, 2, 6]\n",
        "gap_list = [4, 4, 4, 4, 4]\n",
        "model_list = ['LR', 'RF']\n",
        "output_classes = 2\n",
        "\n",
        "# empty result_dict\n",
        "result_dict = {}\n",
        "\n",
        "# load data\n",
        "data_label = np.load(datapath, allow_pickle=True).item()\n",
        "train_head = data_label['train_head']\n",
        "static_train_filter = data_label['static_train_filter']\n",
        "dev_head = data_label['dev_head']\n",
        "static_dev_filter = data_label['static_dev_filter']\n",
        "test_head = data_label['test_head']\n",
        "static_test_filter = data_label['static_test_filter']\n",
        "s_train = np.stack(static_train_filter, axis=0)\n",
        "s_dev = np.stack(static_dev_filter, axis=0)\n",
        "s_test = np.stack(static_test_filter, axis=0)\n",
        "\n",
        "# load cross validation data \n",
        "data_label = np.load(datapath_cv, allow_pickle=True).item()\n",
        "etrain_head = data_label['train_head']\n",
        "estatic_train_filter = data_label['static_train_filter']\n",
        "edev_head = data_label['dev_head']\n",
        "estatic_dev_filter = data_label['static_dev_filter']\n",
        "etest_head = data_label['test_head']\n",
        "estatic_test_filter = data_label['static_test_filter']\n",
        "es_train = np.stack(estatic_train_filter, axis=0)\n",
        "es_dev = np.stack(estatic_dev_filter, axis=0)\n",
        "es_test = np.stack(estatic_test_filter, axis=0) \n",
        "\n",
        "for m in range(2): # model choices \n",
        "    args.model_name = model_list[m]\n",
        "    for ii in range(0, 5): # tasks \n",
        "        result_dict = {}\n",
        "        args.thresh = thresh_list[ii]\n",
        "        args.target_index = target_list[ii]\n",
        "        args.gap = gap_list[ii]\n",
        "\n",
        "        print('Running target %d, thresh %d, gap %d, model %s'%(args.target_index, args.thresh, args.gap, args.model_name))\n",
        "        workname = date + args.model_name + '_target_%d'%args.target_index + '_%dh'%args.thresh + '_%dh'%args.gap\n",
        "        print(workname)\n",
        "\n",
        "        if args.target_index == 0 and args.filter_los == True:\n",
        "            print('Before filtering, train size is %d'%(len(train_head)))\n",
        "            train_label, train_data = filter_los(s_train, train_head, args.thresh, args.gap)\n",
        "            dev_label, dev_data = filter_los(s_dev, dev_head, args.thresh, args.gap)\n",
        "            test_label, test_data = filter_los(s_test, test_head, args.thresh, args.gap)\n",
        "            print('After filtering, train size is %d'%(len(train_data)))\n",
        "            train_label = train_label[:, 0]\n",
        "            dev_label = dev_label[:, 0]\n",
        "            test_label = test_label[:, 0]\n",
        "\n",
        "        if args.target_index == 1 : # arf\n",
        "            print('Before filtering, train size is %d'%(len(train_head)))\n",
        "            train_data, train_label = filter_arf(args, train_head)\n",
        "            dev_data, dev_label= filter_arf(args, dev_head)\n",
        "            test_data, test_label = filter_arf(args, test_head)\n",
        "            print('After filtering, train size is %d'%(len(train_data)))\n",
        "\n",
        "        if args.target_index == 2 : # shock\n",
        "            print('Before filtering, train size is %d'%(len(train_head)))\n",
        "            train_data, train_label = filter_shock(args, train_head)\n",
        "            dev_data, dev_label = filter_shock(args, dev_head)\n",
        "            test_data, test_label = filter_shock(args, test_head)\n",
        "            print('After filtering, train size is %d'%(len(train_data)))\n",
        "\n",
        "        trainval_target = np.concatenate((train_label, dev_label), axis=0).astype(int)\n",
        "        trainval_data = np.stack(train_data + dev_data, axis=0).reshape((len(trainval_target), -1))\n",
        "\n",
        "        # generate weight for the imbalanced label distirbution \n",
        "        if args.model_name == 'LR':\n",
        "            def bo_tune_rf(C, l1_ratio):\n",
        "                regressor = LogisticRegression(penalty='elasticnet', dual=False, C=C, fit_intercept=True, intercept_scaling=1, \\\n",
        "                                                class_weight='balanced', random_state=0, solver='saga', \\\n",
        "                                                verbose=0, warm_start=False, n_jobs=None, l1_ratio=l1_ratio)\n",
        "            \n",
        "                scores = cross_val_score(regressor, trainval_data, trainval_target, cv=5, scoring='roc_auc')\n",
        "                return np.mean(scores)\n",
        "\n",
        "            #Invoking the Bayesian Optimizer with the specified parameters to tune\n",
        "            xgb_bo = BayesianOptimization(bo_tune_rf, {\n",
        "                                                'C' : (0.01, 5), \n",
        "                                                'l1_ratio': (0, 1)})\n",
        "\n",
        "            #performing Bayesian optimization for 5 iterations with 8 steps of random exploration with an #acquisition function of expected improvement\n",
        "            xgb_bo.maximize(n_iter=10, init_points=5, acq='ei')\n",
        "            #   0.9368   |  0.745    |  620.2    |  0.00842  C     | l1_ratio  | max_iter  |    tol  \n",
        "            p_dict = xgb_bo.max['params']\n",
        "            # p_dict = {'C': 0.1803, 'l1_ratio': 0.9982}\n",
        "            classifier = LogisticRegression(penalty='elasticnet', dual=False, C =p_dict['C'], fit_intercept=True, intercept_scaling=1, \\\n",
        "                                                class_weight='balanced', random_state=0, solver='saga', \\\n",
        "                                                verbose=0, warm_start=False, n_jobs=None, l1_ratio=p_dict['l1_ratio'])\n",
        "            classifier.fit(trainval_data, trainval_target)\n",
        "\n",
        "\n",
        "        elif args.model_name == 'RF':\n",
        "            \n",
        "            #Baysian Optimization \n",
        "        # 'max_depth': 6, 'eta':0.3, 'objective':'reg:squarederror', 'num_parallel_tree': 100, 'subsample': 0.5, \n",
        "        #           'reg_lambda': 2, 'reg_alpha':1, 'colsample_bytree': 0.5\n",
        "            def bo_tune_rf(n_estimators, max_depth, min_samples_split, min_samples_leaf, max_features, max_samples):\n",
        "\n",
        "                regressor = RandomForestClassifier(n_estimators=int(n_estimators), max_depth=int(max_depth), \\\n",
        "                                                    min_samples_split= int(min_samples_split), min_samples_leaf=int(min_samples_leaf), \\\n",
        "                                                    max_features = max_features, \\\n",
        "                                                    random_state=0, verbose=0, max_samples=max_samples, class_weight='balanced')\n",
        "                \n",
        "                scores = cross_val_score(regressor, trainval_data, trainval_target, cv=2, scoring='roc_auc')\n",
        "                # cv_result = xgb.cv(params, dtrain, num_boost_round=int(num_round), nfold=5)\n",
        "                #Return the negative RMSE\n",
        "                return np.mean(scores)\n",
        "\n",
        "            #Invoking the Bayesian Optimizer with the specified parameters to tune\n",
        "            xgb_bo = BayesianOptimization(bo_tune_rf, {\n",
        "                                                'n_estimators' : (50, 200),\n",
        "                                                'max_depth': (3, 30), \n",
        "                                                'min_samples_split': (2, 11),  \n",
        "                                                'min_samples_leaf': (1, 11), \n",
        "                                                'max_features': (0.2, 1), \n",
        "                                                'max_samples': (0.2, 1)})\n",
        "\n",
        "            #performing Bayesian optimization for 5 iterations with 8 steps of random exploration with an #acquisition function of expected improvement\n",
        "            xgb_bo.maximize(n_iter=10, init_points=5, acq='ei')\n",
        "        \n",
        "            p_dict = xgb_bo.max['params']\n",
        "        #   |  0.8618   |  13.71    |  0.2364   |  0.5648   |  9.199    |  3.71     |  176.6    |\n",
        "            # p_dict = {'n_estimators': 176.6, 'max_depth': 13.71, 'min_samples_split': 3.71, 'min_samples_leaf': 9.199, 'max_features': 0.2364, 'max_samples':0.5648}\n",
        "\n",
        "            classifier = RandomForestClassifier(n_estimators=int(p_dict['n_estimators']), max_depth=int(p_dict['max_depth']), \\\n",
        "                                                        min_samples_split= int(p_dict['min_samples_split']), min_samples_leaf=int(p_dict['min_samples_leaf']), \\\n",
        "                                                        max_features = p_dict['max_features'], \\\n",
        "                                                        random_state=0, verbose=0, max_samples=p_dict['max_samples'], class_weight='balanced')\n",
        "            classifier.fit(trainval_data, trainval_target)\n",
        "\n",
        "        ## bootstrapping on its own testset \n",
        "        roc = []\n",
        "        prc = []\n",
        "        for i in tqdm(range(1000)):\n",
        "            test_index = np.random.choice(len(test_label), 1000)\n",
        "            test_i = [test_data[i] for i in test_index]\n",
        "            test_t = test_label[test_index].astype(int)\n",
        "            test_stack = np.stack(test_i).reshape((len(test_t), -1))\n",
        "            test_pred_prob= classifier.predict_proba(test_stack)\n",
        "            test_roc = roc_auc_score(test_t, test_pred_prob[:, 1])\n",
        "            test_prc = average_precision_score(test_t, test_pred_prob[:, 1])\n",
        "            roc.append(test_roc)\n",
        "            prc.append(test_prc)\n",
        "\n",
        "        #create 95% confidence interval for population mean weight\n",
        "        print('Running target %d, thresh %d'%(args.target_index, args.thresh))\n",
        "        print('%.3f'%np.mean(roc))\n",
        "        print('(%.3f-%.3f)'%st.t.interval(alpha=0.95, df=len(roc), loc=np.mean(roc), scale=np.std(roc)))\n",
        "        print('%.3f'%np.mean(prc))\n",
        "        print('(%.3f-%.3f)'%st.t.interval(alpha=0.95, df=len(prc), loc=np.mean(prc), scale=np.std(prc)))\n",
        "        result_dict[workname] = ['%.3f'%np.mean(roc)]\n",
        "        result_dict[workname].append('(%.3f-%.3f)'%st.t.interval(alpha=0.95, df=len(roc), loc=np.mean(roc), scale=np.std(roc)))\n",
        "        result_dict[workname].append('%.3f'%np.mean(prc))\n",
        "        result_dict[workname].append('(%.3f-%.3f)'%st.t.interval(alpha=0.95, df=len(prc), loc=np.mean(prc), scale=np.std(prc)))\n",
        "        result_dict[workname].append(len(test_label))\n",
        "\n",
        "        # cross validation on the other dataset \n",
        "        if args.target_index == 0 and args.filter_los == True:\n",
        "            print('Before filtering, train size is %d'%(len(etrain_head)))\n",
        "            es_train, etrain_data = filter_los(es_train, etrain_head, args.thresh, args.gap)\n",
        "            es_dev, edev_data = filter_los(es_dev, edev_head, args.thresh, args.gap)\n",
        "            es_test, etest_data = filter_los(es_test, etest_head, args.thresh, args.gap)\n",
        "            print('After filtering, train size is %d'%(len(etrain_head)))\n",
        "            etrain_label = es_train[:, 0]\n",
        "            edev_label= es_dev[:, 0]\n",
        "            etest_label = es_test[:, 0]\n",
        "\n",
        "        elif args.target_index == 1:\n",
        "            etrain_data, etrain_label = filter_arf(args, etrain_head)\n",
        "            edev_data, edev_label = filter_arf(args, edev_head)\n",
        "            etest_data, etest_label = filter_arf(args, etest_head)\n",
        "\n",
        "        elif args.target_index == 2:\n",
        "            etrain_data, etrain_label = filter_shock(args, etrain_head)\n",
        "            edev_data, edev_label = filter_shock(args, edev_head)\n",
        "            etest_data, etest_label = filter_shock(args, etest_head)\n",
        "\n",
        "        crossval_target = np.concatenate((etrain_label, edev_label, etest_label), axis= 0)\n",
        "        crossval_head = np.stack(etrain_data + edev_data + etest_data, axis=0)\n",
        "\n",
        "        roc = []\n",
        "        prc = []\n",
        "        for i in tqdm(range(1000)):\n",
        "            test_index = np.random.choice(len(crossval_target), 1000)\n",
        "            test_i = [crossval_head[i] for i in test_index]\n",
        "            test_t = crossval_target[test_index].astype(int)\n",
        "            test_stack = np.stack(test_i).reshape((len(test_t), -1))\n",
        "            test_pred_prob= classifier.predict_proba(test_stack)\n",
        "            test_roc = roc_auc_score(test_t, test_pred_prob[:, 1])\n",
        "            test_prc = average_precision_score(test_t, test_pred_prob[:, 1])\n",
        "            roc.append(test_roc)\n",
        "            prc.append(test_prc)\n",
        "        #create 95% confidence interval for population mean weight\n",
        "        print('%.3f'%np.mean(roc))\n",
        "        print('(%.3f-%.3f)'%st.t.interval(alpha=0.95, df=len(roc), loc=np.mean(roc), scale=np.std(roc)))\n",
        "        print('%.3f'%np.mean(prc))\n",
        "        print('(%.3f-%.3f)'%st.t.interval(alpha=0.95, df=len(prc), loc=np.mean(prc), scale=np.std(prc)))\n",
        "        result_dict[workname].append('%.3f'%np.mean(roc))\n",
        "        result_dict[workname].append('(%.3f-%.3f)'%st.t.interval(alpha=0.95, df=len(roc), loc=np.mean(roc), scale=np.std(roc)))\n",
        "        result_dict[workname].append('%.3f'%np.mean(prc))\n",
        "        result_dict[workname].append('(%.3f-%.3f)'%st.t.interval(alpha=0.95, df=len(prc), loc=np.mean(prc), scale=np.std(prc)))    \n",
        "        result_dict[workname].append(len(crossval_target))\n",
        "        print(result_dict) \n",
        "\n",
        "        # figure importance \n",
        "        LR models\n",
        "        if args.model_name == 'LR':\n",
        "            a = classifier.coef_\n",
        "            a_pos = np.abs(a)\n",
        "            c = a_pos.reshape(200, int(args.thresh)).T\n",
        "            d = c.sum(axis=0)\n",
        "            rf_mimic_IM = pd.DataFrame({'name': index, 'im':d})\n",
        "            rf_mimic_IM.sort_values(by=['im'], ascending=False, inplace=True)\n",
        "            rf_mimic_IM.to_hdf(os.path.join('./checkpoints/', 'MEEP_eICU_LRRF.h5'), key=workname)\n",
        "            fig, ax = plt.subplots()\n",
        "            im = ax.imshow(c, cmap=Dense_20.mpl_colormap, vmax = 5e-2, label='23 h')\n",
        "            divider = make_axes_locatable(ax)\n",
        "            cax = divider.append_axes('right', size='5%', pad=0.05)\n",
        "            fig.colorbar(im, cax=cax, orientation='vertical')\n",
        "            im.set_label('Label via method')\n",
        "            # ax.legend([], ['23h'], loc='upper left', bbox_to_anchor=(0, 1.1), prop=legend_properties)\n",
        "            # ax.set_title('RF model', size=12,  fontweight='bold')\n",
        "            ax.grid('off')\n",
        "            ax.set_ylabel('ICU_in Hours', size=12,  fontweight='bold')\n",
        "            ax.set_xlabel('Feature Index', size=12,  fontweight='bold')\n",
        "            plt.savefig('./checkpoints/' + workname + '_feature_importance_eicu.eps',format='eps', bbox_inches = 'tight', pad_inches = 0.1)\n",
        "            plt.show()\n",
        "\n",
        "        else: \n",
        "            a_pos = classifier.feature_importances_\n",
        "            c = a_pos.reshape(200, int(args.thresh)).T\n",
        "            d = c.sum(axis=0)\n",
        "            rf_mimic_IM = pd.DataFrame({'name': index, 'im':d}) \n",
        "            rf_mimic_IM.to_hdf(os.path.join('./checkpoints/', 'MEEP_eICU_LRRF.h5'), key=workname)\n",
        "            fig, ax = plt.subplots()\n",
        "            im = ax.imshow(c, cmap=Dense_20.mpl_colormap, vmax = 5e-3, label='23 h')\n",
        "            divider = make_axes_locatable(ax)\n",
        "            cax = divider.append_axes('right', size='5%', pad=0.05)\n",
        "            fig.colorbar(im, cax=cax, orientation='vertical')\n",
        "            im.set_label('Label via method')\n",
        "            # ax.legend([], ['23h'], loc='upper left', bbox_to_anchor=(0, 1.1), prop=legend_properties)\n",
        "            # ax.set_title('RF model', size=12,  fontweight='bold')\n",
        "            ax.grid('off')\n",
        "            ax.set_ylabel('ICU_in Hours', size=12,  fontweight='bold')\n",
        "            ax.set_xlabel('Feature Index', size=12,  fontweight='bold')\n",
        "            plt.savefig('./checkpoints/' + workname + '_feature_importance_eicu.eps',format='eps', bbox_inches = 'tight', pad_inches = 0.1)\n",
        "            plt.show()\n",
        "        \n",
        "        ## auc curves \n",
        "        test_stack = np.stack(test_data).reshape((len(test_label), -1))\n",
        "        test_pred_prob= classifier.predict_proba(test_stack)\n",
        "        test_stack_e = np.stack(crossval_head).reshape((len(crossval_target), -1))\n",
        "        test_pred_prob_e= classifier.predict_proba(test_stack_e)\n",
        "\n",
        "        ratio_1 = 1000/len(test_label)\n",
        "        ratio_2 = 1000/len(crossval_target)\n",
        "        rocs = rr._roc.compute_roc_bootstrap(X=test_pred_prob[:, 1], y=test_label, frac = ratio_1, pos_label=1,\n",
        "                                    n_bootstrap=1000,\n",
        "                                random_state=42,\n",
        "                                    return_mean=False)\n",
        "        rocs_1 = rr._roc.compute_roc_bootstrap(X=test_pred_prob_e[:, 1], y=crossval_target, frac= ratio_2, pos_label=1,\n",
        "                                    n_bootstrap=1000,\n",
        "                                    random_state=42,\n",
        "                                    return_mean=False) \n",
        "        plot_mean_roc_2(rocs, rocs_1, legend='%s, %s'%(args.model_name, task_map[args.target_index]), workname = workname + '_eicu', show_ci=True, show_ti=True)\n",
        "\n",
        "\n",
        "        write_json('./checkpoints', workname + '_mimic.json', result_dict)"
      ],
      "metadata": {
        "id": "L8GtzrmgTG9q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vfv0bDLwlQ_o",
        "outputId": "fc2a8605-7277-4f46-ef60-335257a1ba0a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running target 0, thresh 24, gap 4, model RF\n",
            "0228RF_target_0_24h_4h\n",
            "Before filtering, train size is 27136\n",
            "After filtering, train size is 12768\n",
            "|   iter    |  target   | max_depth | max_fe... | max_sa... | min_sa... | min_sa... | n_esti... |\n",
            "-------------------------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-5d726b46a94a>:93: DeprecationWarning: \n",
            "Passing acquisition function parameters or gaussian process parameters to maximize\n",
            "is no longer supported, and will cause an error in future releases. Instead,\n",
            "please use the \"set_gp_params\" method to set the gp params, and pass an instance\n",
            " of bayes_opt.util.UtilityFunction using the acquisition_function argument\n",
            "\n",
            "  xgb_bo.maximize(n_iter=10, init_points=5, acq='ei')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| \u001b[0m1        \u001b[0m | \u001b[0m0.8586   \u001b[0m | \u001b[0m18.32    \u001b[0m | \u001b[0m0.909    \u001b[0m | \u001b[0m0.4668   \u001b[0m | \u001b[0m10.93    \u001b[0m | \u001b[0m6.948    \u001b[0m | \u001b[0m60.84    \u001b[0m |\n",
            "| \u001b[0m2        \u001b[0m | \u001b[0m0.8481   \u001b[0m | \u001b[0m8.833    \u001b[0m | \u001b[0m0.8579   \u001b[0m | \u001b[0m0.3113   \u001b[0m | \u001b[0m1.302    \u001b[0m | \u001b[0m9.166    \u001b[0m | \u001b[0m68.48    \u001b[0m |\n",
            "| \u001b[95m3        \u001b[0m | \u001b[95m0.86     \u001b[0m | \u001b[95m10.41    \u001b[0m | \u001b[95m0.6027   \u001b[0m | \u001b[95m0.8276   \u001b[0m | \u001b[95m6.539    \u001b[0m | \u001b[95m8.28     \u001b[0m | \u001b[95m88.17    \u001b[0m |\n",
            "| \u001b[0m4        \u001b[0m | \u001b[0m0.8253   \u001b[0m | \u001b[0m3.306    \u001b[0m | \u001b[0m0.6078   \u001b[0m | \u001b[0m0.2407   \u001b[0m | \u001b[0m10.76    \u001b[0m | \u001b[0m3.839    \u001b[0m | \u001b[0m75.89    \u001b[0m |\n",
            "| \u001b[0m5        \u001b[0m | \u001b[0m0.8559   \u001b[0m | \u001b[0m20.41    \u001b[0m | \u001b[0m0.8732   \u001b[0m | \u001b[0m0.3598   \u001b[0m | \u001b[0m3.043    \u001b[0m | \u001b[0m3.645    \u001b[0m | \u001b[0m122.4    \u001b[0m |\n",
            "| \u001b[95m6        \u001b[0m | \u001b[95m0.865    \u001b[0m | \u001b[95m19.65    \u001b[0m | \u001b[95m0.4684   \u001b[0m | \u001b[95m0.9004   \u001b[0m | \u001b[95m10.47    \u001b[0m | \u001b[95m6.391    \u001b[0m | \u001b[95m60.56    \u001b[0m |\n",
            "| \u001b[0m7        \u001b[0m | \u001b[0m0.8631   \u001b[0m | \u001b[0m24.18    \u001b[0m | \u001b[0m0.2      \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m8.904    \u001b[0m | \u001b[0m4.571    \u001b[0m | \u001b[0m59.62    \u001b[0m |\n",
            "| \u001b[0m8        \u001b[0m | \u001b[0m0.8603   \u001b[0m | \u001b[0m18.85    \u001b[0m | \u001b[0m0.7133   \u001b[0m | \u001b[0m0.6355   \u001b[0m | \u001b[0m7.32     \u001b[0m | \u001b[0m2.439    \u001b[0m | \u001b[0m56.26    \u001b[0m |\n",
            "| \u001b[0m9        \u001b[0m | \u001b[0m0.8611   \u001b[0m | \u001b[0m23.05    \u001b[0m | \u001b[0m0.2      \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m10.57    \u001b[0m | \u001b[0m10.09    \u001b[0m | \u001b[0m55.28    \u001b[0m |\n",
            "| \u001b[0m10       \u001b[0m | \u001b[0m0.8579   \u001b[0m | \u001b[0m22.19    \u001b[0m | \u001b[0m0.2      \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m4.077    \u001b[0m | \u001b[0m9.574    \u001b[0m | \u001b[0m61.57    \u001b[0m |\n",
            "| \u001b[95m11       \u001b[0m | \u001b[95m0.865    \u001b[0m | \u001b[95m23.11    \u001b[0m | \u001b[95m0.6149   \u001b[0m | \u001b[95m0.8906   \u001b[0m | \u001b[95m9.927    \u001b[0m | \u001b[95m2.942    \u001b[0m | \u001b[95m66.43    \u001b[0m |\n",
            "| \u001b[0m12       \u001b[0m | \u001b[0m0.8646   \u001b[0m | \u001b[0m29.94    \u001b[0m | \u001b[0m0.2478   \u001b[0m | \u001b[0m0.7085   \u001b[0m | \u001b[0m10.63    \u001b[0m | \u001b[0m8.376    \u001b[0m | \u001b[0m69.59    \u001b[0m |\n",
            "| \u001b[95m13       \u001b[0m | \u001b[95m0.8663   \u001b[0m | \u001b[95m28.23    \u001b[0m | \u001b[95m0.2      \u001b[0m | \u001b[95m1.0      \u001b[0m | \u001b[95m11.0     \u001b[0m | \u001b[95m2.0      \u001b[0m | \u001b[95m75.14    \u001b[0m |\n",
            "| \u001b[0m14       \u001b[0m | \u001b[0m0.8504   \u001b[0m | \u001b[0m29.73    \u001b[0m | \u001b[0m0.364    \u001b[0m | \u001b[0m0.2447   \u001b[0m | \u001b[0m3.466    \u001b[0m | \u001b[0m3.251    \u001b[0m | \u001b[0m72.38    \u001b[0m |\n",
            "| \u001b[0m15       \u001b[0m | \u001b[0m0.863    \u001b[0m | \u001b[0m24.31    \u001b[0m | \u001b[0m0.6314   \u001b[0m | \u001b[0m0.4757   \u001b[0m | \u001b[0m10.96    \u001b[0m | \u001b[0m6.743    \u001b[0m | \u001b[0m77.3     \u001b[0m |\n",
            "=================================================================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1000/1000 [01:24<00:00, 11.87it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running target 0, thresh 24\n",
            "0.870\n",
            "(0.838-0.902)\n",
            "0.498\n",
            "(0.399-0.597)\n",
            "{'0228RF_target_0_24h_4h': ['0.870', '(0.838-0.902)', '0.498', '(0.399-0.597)', 3691]}\n",
            "Running target 1, thresh 2, gap 4, model RF\n",
            "0228RF_target_1_2h_4h\n",
            "Before filtering, train size is 27136\n",
            "After filtering, train size is 6544\n",
            "|   iter    |  target   | max_depth | max_fe... | max_sa... | min_sa... | min_sa... | n_esti... |\n",
            "-------------------------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-5d726b46a94a>:93: DeprecationWarning: \n",
            "Passing acquisition function parameters or gaussian process parameters to maximize\n",
            "is no longer supported, and will cause an error in future releases. Instead,\n",
            "please use the \"set_gp_params\" method to set the gp params, and pass an instance\n",
            " of bayes_opt.util.UtilityFunction using the acquisition_function argument\n",
            "\n",
            "  xgb_bo.maximize(n_iter=10, init_points=5, acq='ei')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| \u001b[0m1        \u001b[0m | \u001b[0m0.7741   \u001b[0m | \u001b[0m28.21    \u001b[0m | \u001b[0m0.7968   \u001b[0m | \u001b[0m0.5785   \u001b[0m | \u001b[0m5.995    \u001b[0m | \u001b[0m3.6      \u001b[0m | \u001b[0m145.1    \u001b[0m |\n",
            "| \u001b[0m2        \u001b[0m | \u001b[0m0.7705   \u001b[0m | \u001b[0m11.04    \u001b[0m | \u001b[0m0.6663   \u001b[0m | \u001b[0m0.566    \u001b[0m | \u001b[0m6.885    \u001b[0m | \u001b[0m7.586    \u001b[0m | \u001b[0m59.52    \u001b[0m |\n",
            "| \u001b[0m3        \u001b[0m | \u001b[0m0.7725   \u001b[0m | \u001b[0m24.08    \u001b[0m | \u001b[0m0.9579   \u001b[0m | \u001b[0m0.5575   \u001b[0m | \u001b[0m10.2     \u001b[0m | \u001b[0m5.917    \u001b[0m | \u001b[0m77.73    \u001b[0m |\n",
            "| \u001b[0m4        \u001b[0m | \u001b[0m0.7738   \u001b[0m | \u001b[0m28.66    \u001b[0m | \u001b[0m0.7178   \u001b[0m | \u001b[0m0.9257   \u001b[0m | \u001b[0m7.813    \u001b[0m | \u001b[0m7.592    \u001b[0m | \u001b[0m184.2    \u001b[0m |\n",
            "| \u001b[95m5        \u001b[0m | \u001b[95m0.7814   \u001b[0m | \u001b[95m18.73    \u001b[0m | \u001b[95m0.5525   \u001b[0m | \u001b[95m0.3441   \u001b[0m | \u001b[95m7.978    \u001b[0m | \u001b[95m7.582    \u001b[0m | \u001b[95m187.3    \u001b[0m |\n",
            "| \u001b[0m6        \u001b[0m | \u001b[0m0.7787   \u001b[0m | \u001b[0m18.92    \u001b[0m | \u001b[0m0.8921   \u001b[0m | \u001b[0m0.2213   \u001b[0m | \u001b[0m7.27     \u001b[0m | \u001b[0m7.552    \u001b[0m | \u001b[0m186.7    \u001b[0m |\n",
            "| \u001b[0m7        \u001b[0m | \u001b[0m0.7785   \u001b[0m | \u001b[0m16.33    \u001b[0m | \u001b[0m0.4019   \u001b[0m | \u001b[0m0.932    \u001b[0m | \u001b[0m8.219    \u001b[0m | \u001b[0m6.588    \u001b[0m | \u001b[0m187.1    \u001b[0m |\n",
            "| \u001b[0m8        \u001b[0m | \u001b[0m0.7808   \u001b[0m | \u001b[0m18.47    \u001b[0m | \u001b[0m0.2      \u001b[0m | \u001b[0m0.7361   \u001b[0m | \u001b[0m9.25     \u001b[0m | \u001b[0m7.712    \u001b[0m | \u001b[0m188.8    \u001b[0m |\n",
            "| \u001b[0m9        \u001b[0m | \u001b[0m0.777    \u001b[0m | \u001b[0m18.43    \u001b[0m | \u001b[0m0.7576   \u001b[0m | \u001b[0m0.5526   \u001b[0m | \u001b[0m10.66    \u001b[0m | \u001b[0m9.735    \u001b[0m | \u001b[0m187.0    \u001b[0m |\n",
            "| \u001b[0m10       \u001b[0m | \u001b[0m0.7761   \u001b[0m | \u001b[0m19.66    \u001b[0m | \u001b[0m0.8299   \u001b[0m | \u001b[0m0.7951   \u001b[0m | \u001b[0m7.625    \u001b[0m | \u001b[0m7.117    \u001b[0m | \u001b[0m189.7    \u001b[0m |\n",
            "| \u001b[0m11       \u001b[0m | \u001b[0m0.7808   \u001b[0m | \u001b[0m20.4     \u001b[0m | \u001b[0m0.2756   \u001b[0m | \u001b[0m0.2101   \u001b[0m | \u001b[0m10.1     \u001b[0m | \u001b[0m6.291    \u001b[0m | \u001b[0m186.9    \u001b[0m |\n",
            "| \u001b[95m12       \u001b[0m | \u001b[95m0.7817   \u001b[0m | \u001b[95m17.03    \u001b[0m | \u001b[95m0.6242   \u001b[0m | \u001b[95m0.3645   \u001b[0m | \u001b[95m10.68    \u001b[0m | \u001b[95m4.927    \u001b[0m | \u001b[95m186.7    \u001b[0m |\n",
            "| \u001b[0m13       \u001b[0m | \u001b[0m0.7781   \u001b[0m | \u001b[0m17.8     \u001b[0m | \u001b[0m0.3832   \u001b[0m | \u001b[0m0.6484   \u001b[0m | \u001b[0m10.16    \u001b[0m | \u001b[0m4.519    \u001b[0m | \u001b[0m183.8    \u001b[0m |\n",
            "| \u001b[0m14       \u001b[0m | \u001b[0m0.777    \u001b[0m | \u001b[0m18.17    \u001b[0m | \u001b[0m0.5804   \u001b[0m | \u001b[0m0.8297   \u001b[0m | \u001b[0m10.88    \u001b[0m | \u001b[0m3.436    \u001b[0m | \u001b[0m188.9    \u001b[0m |\n",
            "| \u001b[0m15       \u001b[0m | \u001b[0m0.777    \u001b[0m | \u001b[0m17.78    \u001b[0m | \u001b[0m0.7936   \u001b[0m | \u001b[0m0.7199   \u001b[0m | \u001b[0m10.59    \u001b[0m | \u001b[0m6.785    \u001b[0m | \u001b[0m184.8    \u001b[0m |\n",
            "=================================================================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1000/1000 [01:20<00:00, 12.50it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running target 1, thresh 2\n",
            "0.770\n",
            "(0.734-0.806)\n",
            "0.618\n",
            "(0.561-0.676)\n",
            "{'0228RF_target_1_2h_4h': ['0.770', '(0.734-0.806)', '0.618', '(0.561-0.676)', 1900]}\n",
            "Running target 1, thresh 6, gap 4, model RF\n",
            "0228RF_target_1_6h_4h\n",
            "Before filtering, train size is 27136\n",
            "After filtering, train size is 5902\n",
            "|   iter    |  target   | max_depth | max_fe... | max_sa... | min_sa... | min_sa... | n_esti... |\n",
            "-------------------------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-5d726b46a94a>:93: DeprecationWarning: \n",
            "Passing acquisition function parameters or gaussian process parameters to maximize\n",
            "is no longer supported, and will cause an error in future releases. Instead,\n",
            "please use the \"set_gp_params\" method to set the gp params, and pass an instance\n",
            " of bayes_opt.util.UtilityFunction using the acquisition_function argument\n",
            "\n",
            "  xgb_bo.maximize(n_iter=10, init_points=5, acq='ei')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| \u001b[0m1        \u001b[0m | \u001b[0m0.7831   \u001b[0m | \u001b[0m22.34    \u001b[0m | \u001b[0m0.4763   \u001b[0m | \u001b[0m0.3603   \u001b[0m | \u001b[0m7.92     \u001b[0m | \u001b[0m6.237    \u001b[0m | \u001b[0m171.4    \u001b[0m |\n",
            "| \u001b[0m2        \u001b[0m | \u001b[0m0.7714   \u001b[0m | \u001b[0m10.75    \u001b[0m | \u001b[0m0.502    \u001b[0m | \u001b[0m0.7251   \u001b[0m | \u001b[0m5.563    \u001b[0m | \u001b[0m8.271    \u001b[0m | \u001b[0m124.5    \u001b[0m |\n",
            "| \u001b[0m3        \u001b[0m | \u001b[0m0.7719   \u001b[0m | \u001b[0m21.12    \u001b[0m | \u001b[0m0.7519   \u001b[0m | \u001b[0m0.8196   \u001b[0m | \u001b[0m5.0      \u001b[0m | \u001b[0m2.724    \u001b[0m | \u001b[0m92.64    \u001b[0m |\n",
            "| \u001b[0m4        \u001b[0m | \u001b[0m0.7795   \u001b[0m | \u001b[0m9.621    \u001b[0m | \u001b[0m0.2691   \u001b[0m | \u001b[0m0.2651   \u001b[0m | \u001b[0m6.417    \u001b[0m | \u001b[0m6.501    \u001b[0m | \u001b[0m84.96    \u001b[0m |\n",
            "| \u001b[0m5        \u001b[0m | \u001b[0m0.7749   \u001b[0m | \u001b[0m18.45    \u001b[0m | \u001b[0m0.9545   \u001b[0m | \u001b[0m0.3161   \u001b[0m | \u001b[0m5.606    \u001b[0m | \u001b[0m9.793    \u001b[0m | \u001b[0m98.62    \u001b[0m |\n",
            "| \u001b[0m6        \u001b[0m | \u001b[0m0.7802   \u001b[0m | \u001b[0m21.92    \u001b[0m | \u001b[0m0.9797   \u001b[0m | \u001b[0m0.6553   \u001b[0m | \u001b[0m7.688    \u001b[0m | \u001b[0m6.226    \u001b[0m | \u001b[0m172.5    \u001b[0m |\n",
            "| \u001b[0m7        \u001b[0m | \u001b[0m0.7803   \u001b[0m | \u001b[0m21.71    \u001b[0m | \u001b[0m0.322    \u001b[0m | \u001b[0m0.5887   \u001b[0m | \u001b[0m7.582    \u001b[0m | \u001b[0m7.504    \u001b[0m | \u001b[0m169.0    \u001b[0m |\n",
            "| \u001b[0m8        \u001b[0m | \u001b[0m0.7815   \u001b[0m | \u001b[0m25.17    \u001b[0m | \u001b[0m0.5337   \u001b[0m | \u001b[0m0.2539   \u001b[0m | \u001b[0m8.263    \u001b[0m | \u001b[0m6.217    \u001b[0m | \u001b[0m170.5    \u001b[0m |\n",
            "| \u001b[0m9        \u001b[0m | \u001b[0m0.7792   \u001b[0m | \u001b[0m22.33    \u001b[0m | \u001b[0m0.2      \u001b[0m | \u001b[0m0.2      \u001b[0m | \u001b[0m9.626    \u001b[0m | \u001b[0m3.74     \u001b[0m | \u001b[0m169.9    \u001b[0m |\n",
            "| \u001b[0m10       \u001b[0m | \u001b[0m0.7783   \u001b[0m | \u001b[0m23.11    \u001b[0m | \u001b[0m0.2      \u001b[0m | \u001b[0m0.2      \u001b[0m | \u001b[0m10.38    \u001b[0m | \u001b[0m8.095    \u001b[0m | \u001b[0m171.4    \u001b[0m |\n",
            "| \u001b[0m11       \u001b[0m | \u001b[0m0.7792   \u001b[0m | \u001b[0m23.18    \u001b[0m | \u001b[0m0.9529   \u001b[0m | \u001b[0m0.2931   \u001b[0m | \u001b[0m5.013    \u001b[0m | \u001b[0m6.301    \u001b[0m | \u001b[0m170.0    \u001b[0m |\n",
            "| \u001b[0m12       \u001b[0m | \u001b[0m0.7796   \u001b[0m | \u001b[0m25.11    \u001b[0m | \u001b[0m0.4864   \u001b[0m | \u001b[0m0.5291   \u001b[0m | \u001b[0m6.364    \u001b[0m | \u001b[0m2.253    \u001b[0m | \u001b[0m172.7    \u001b[0m |\n",
            "| \u001b[0m13       \u001b[0m | \u001b[0m0.7828   \u001b[0m | \u001b[0m28.52    \u001b[0m | \u001b[0m0.7351   \u001b[0m | \u001b[0m0.5507   \u001b[0m | \u001b[0m7.884    \u001b[0m | \u001b[0m5.121    \u001b[0m | \u001b[0m172.3    \u001b[0m |\n",
            "| \u001b[0m14       \u001b[0m | \u001b[0m0.7778   \u001b[0m | \u001b[0m27.14    \u001b[0m | \u001b[0m0.2      \u001b[0m | \u001b[0m0.2      \u001b[0m | \u001b[0m6.495    \u001b[0m | \u001b[0m6.831    \u001b[0m | \u001b[0m173.6    \u001b[0m |\n",
            "| \u001b[0m15       \u001b[0m | \u001b[0m0.7797   \u001b[0m | \u001b[0m28.6     \u001b[0m | \u001b[0m0.7617   \u001b[0m | \u001b[0m0.7681   \u001b[0m | \u001b[0m8.911    \u001b[0m | \u001b[0m2.67     \u001b[0m | \u001b[0m172.8    \u001b[0m |\n",
            "=================================================================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1000/1000 [01:24<00:00, 11.85it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running target 1, thresh 6\n",
            "0.749\n",
            "(0.703-0.795)\n",
            "0.494\n",
            "(0.417-0.571)\n",
            "{'0228RF_target_1_6h_4h': ['0.749', '(0.703-0.795)', '0.494', '(0.417-0.571)', 1702]}\n",
            "Running target 2, thresh 2, gap 4, model RF\n",
            "0228RF_target_2_2h_4h\n",
            "Before filtering, train size is 27136\n",
            "After filtering, train size is 18933\n",
            "|   iter    |  target   | max_depth | max_fe... | max_sa... | min_sa... | min_sa... | n_esti... |\n",
            "-------------------------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-5d726b46a94a>:93: DeprecationWarning: \n",
            "Passing acquisition function parameters or gaussian process parameters to maximize\n",
            "is no longer supported, and will cause an error in future releases. Instead,\n",
            "please use the \"set_gp_params\" method to set the gp params, and pass an instance\n",
            " of bayes_opt.util.UtilityFunction using the acquisition_function argument\n",
            "\n",
            "  xgb_bo.maximize(n_iter=10, init_points=5, acq='ei')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| \u001b[0m1        \u001b[0m | \u001b[0m0.8819   \u001b[0m | \u001b[0m17.89    \u001b[0m | \u001b[0m0.9723   \u001b[0m | \u001b[0m0.3732   \u001b[0m | \u001b[0m9.815    \u001b[0m | \u001b[0m7.889    \u001b[0m | \u001b[0m147.2    \u001b[0m |\n",
            "| \u001b[0m2        \u001b[0m | \u001b[0m0.8747   \u001b[0m | \u001b[0m14.3     \u001b[0m | \u001b[0m0.6059   \u001b[0m | \u001b[0m0.3388   \u001b[0m | \u001b[0m4.708    \u001b[0m | \u001b[0m2.123    \u001b[0m | \u001b[0m90.85    \u001b[0m |\n",
            "| \u001b[0m3        \u001b[0m | \u001b[0m0.8683   \u001b[0m | \u001b[0m4.231    \u001b[0m | \u001b[0m0.9413   \u001b[0m | \u001b[0m0.8116   \u001b[0m | \u001b[0m6.62     \u001b[0m | \u001b[0m7.354    \u001b[0m | \u001b[0m79.0     \u001b[0m |\n",
            "| \u001b[0m4        \u001b[0m | \u001b[0m0.8801   \u001b[0m | \u001b[0m5.174    \u001b[0m | \u001b[0m0.3534   \u001b[0m | \u001b[0m0.5407   \u001b[0m | \u001b[0m2.209    \u001b[0m | \u001b[0m10.57    \u001b[0m | \u001b[0m121.1    \u001b[0m |\n",
            "| \u001b[95m5        \u001b[0m | \u001b[95m0.8842   \u001b[0m | \u001b[95m16.33    \u001b[0m | \u001b[95m0.4652   \u001b[0m | \u001b[95m0.3621   \u001b[0m | \u001b[95m7.652    \u001b[0m | \u001b[95m8.059    \u001b[0m | \u001b[95m141.6    \u001b[0m |\n",
            "| \u001b[0m6        \u001b[0m | \u001b[0m0.872    \u001b[0m | \u001b[0m29.84    \u001b[0m | \u001b[0m0.7594   \u001b[0m | \u001b[0m0.5468   \u001b[0m | \u001b[0m2.24     \u001b[0m | \u001b[0m2.65     \u001b[0m | \u001b[0m130.7    \u001b[0m |\n",
            "| \u001b[0m7        \u001b[0m | \u001b[0m0.8784   \u001b[0m | \u001b[0m5.266    \u001b[0m | \u001b[0m0.2      \u001b[0m | \u001b[0m0.6778   \u001b[0m | \u001b[0m10.52    \u001b[0m | \u001b[0m11.0     \u001b[0m | \u001b[0m139.3    \u001b[0m |\n",
            "| \u001b[0m8        \u001b[0m | \u001b[0m0.8704   \u001b[0m | \u001b[0m12.36    \u001b[0m | \u001b[0m0.2      \u001b[0m | \u001b[0m0.2      \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m2.831    \u001b[0m | \u001b[0m144.5    \u001b[0m |\n",
            "| \u001b[0m9        \u001b[0m | \u001b[0m0.8829   \u001b[0m | \u001b[0m20.81    \u001b[0m | \u001b[0m0.8275   \u001b[0m | \u001b[0m0.7713   \u001b[0m | \u001b[0m9.745    \u001b[0m | \u001b[0m8.928    \u001b[0m | \u001b[0m141.2    \u001b[0m |\n",
            "| \u001b[0m10       \u001b[0m | \u001b[0m0.8797   \u001b[0m | \u001b[0m15.36    \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m0.2      \u001b[0m | \u001b[0m11.0     \u001b[0m | \u001b[0m11.0     \u001b[0m | \u001b[0m136.6    \u001b[0m |\n",
            "| \u001b[95m11       \u001b[0m | \u001b[95m0.8857   \u001b[0m | \u001b[95m14.05    \u001b[0m | \u001b[95m0.2      \u001b[0m | \u001b[95m1.0      \u001b[0m | \u001b[95m11.0     \u001b[0m | \u001b[95m11.0     \u001b[0m | \u001b[95m144.0    \u001b[0m |\n",
            "| \u001b[0m12       \u001b[0m | \u001b[0m0.8797   \u001b[0m | \u001b[0m5.279    \u001b[0m | \u001b[0m0.5998   \u001b[0m | \u001b[0m0.4374   \u001b[0m | \u001b[0m9.942    \u001b[0m | \u001b[0m10.91    \u001b[0m | \u001b[0m157.2    \u001b[0m |\n",
            "| \u001b[0m13       \u001b[0m | \u001b[0m0.8578   \u001b[0m | \u001b[0m3.013    \u001b[0m | \u001b[0m0.7841   \u001b[0m | \u001b[0m0.9111   \u001b[0m | \u001b[0m10.91    \u001b[0m | \u001b[0m10.75    \u001b[0m | \u001b[0m110.6    \u001b[0m |\n",
            "| \u001b[0m14       \u001b[0m | \u001b[0m0.8842   \u001b[0m | \u001b[0m13.09    \u001b[0m | \u001b[0m0.6061   \u001b[0m | \u001b[0m0.7252   \u001b[0m | \u001b[0m10.64    \u001b[0m | \u001b[0m10.19    \u001b[0m | \u001b[0m170.8    \u001b[0m |\n",
            "| \u001b[0m15       \u001b[0m | \u001b[0m0.8671   \u001b[0m | \u001b[0m3.689    \u001b[0m | \u001b[0m0.6639   \u001b[0m | \u001b[0m0.2907   \u001b[0m | \u001b[0m9.317    \u001b[0m | \u001b[0m8.074    \u001b[0m | \u001b[0m175.2    \u001b[0m |\n",
            "=================================================================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1000/1000 [01:07<00:00, 14.83it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running target 2, thresh 2\n",
            "0.882\n",
            "(0.834-0.930)\n",
            "0.492\n",
            "(0.364-0.621)\n",
            "{'0228RF_target_2_2h_4h': ['0.882', '(0.834-0.930)', '0.492', '(0.364-0.621)', 5405]}\n",
            "Running target 2, thresh 6, gap 4, model RF\n",
            "0228RF_target_2_6h_4h\n",
            "Before filtering, train size is 27136\n",
            "After filtering, train size is 18518\n",
            "|   iter    |  target   | max_depth | max_fe... | max_sa... | min_sa... | min_sa... | n_esti... |\n",
            "-------------------------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-5d726b46a94a>:93: DeprecationWarning: \n",
            "Passing acquisition function parameters or gaussian process parameters to maximize\n",
            "is no longer supported, and will cause an error in future releases. Instead,\n",
            "please use the \"set_gp_params\" method to set the gp params, and pass an instance\n",
            " of bayes_opt.util.UtilityFunction using the acquisition_function argument\n",
            "\n",
            "  xgb_bo.maximize(n_iter=10, init_points=5, acq='ei')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| \u001b[0m1        \u001b[0m | \u001b[0m0.8842   \u001b[0m | \u001b[0m15.52    \u001b[0m | \u001b[0m0.9059   \u001b[0m | \u001b[0m0.8258   \u001b[0m | \u001b[0m6.067    \u001b[0m | \u001b[0m6.355    \u001b[0m | \u001b[0m71.14    \u001b[0m |\n",
            "| \u001b[0m2        \u001b[0m | \u001b[0m0.8804   \u001b[0m | \u001b[0m18.67    \u001b[0m | \u001b[0m0.3846   \u001b[0m | \u001b[0m0.2415   \u001b[0m | \u001b[0m3.146    \u001b[0m | \u001b[0m7.265    \u001b[0m | \u001b[0m114.0    \u001b[0m |\n",
            "| \u001b[95m3        \u001b[0m | \u001b[95m0.8909   \u001b[0m | \u001b[95m26.23    \u001b[0m | \u001b[95m0.4325   \u001b[0m | \u001b[95m0.6636   \u001b[0m | \u001b[95m6.098    \u001b[0m | \u001b[95m5.621    \u001b[0m | \u001b[95m118.0    \u001b[0m |\n",
            "| \u001b[0m4        \u001b[0m | \u001b[0m0.8874   \u001b[0m | \u001b[0m14.79    \u001b[0m | \u001b[0m0.9832   \u001b[0m | \u001b[0m0.635    \u001b[0m | \u001b[0m6.584    \u001b[0m | \u001b[0m3.954    \u001b[0m | \u001b[0m63.53    \u001b[0m |\n",
            "| \u001b[0m5        \u001b[0m | \u001b[0m0.8789   \u001b[0m | \u001b[0m17.68    \u001b[0m | \u001b[0m0.3369   \u001b[0m | \u001b[0m0.7721   \u001b[0m | \u001b[0m1.889    \u001b[0m | \u001b[0m7.029    \u001b[0m | \u001b[0m76.51    \u001b[0m |\n",
            "| \u001b[0m6        \u001b[0m | \u001b[0m0.8864   \u001b[0m | \u001b[0m25.39    \u001b[0m | \u001b[0m0.5729   \u001b[0m | \u001b[0m0.789    \u001b[0m | \u001b[0m4.943    \u001b[0m | \u001b[0m5.894    \u001b[0m | \u001b[0m117.9    \u001b[0m |\n",
            "| \u001b[95m7        \u001b[0m | \u001b[95m0.8944   \u001b[0m | \u001b[95m26.83    \u001b[0m | \u001b[95m0.5221   \u001b[0m | \u001b[95m0.9781   \u001b[0m | \u001b[95m7.11     \u001b[0m | \u001b[95m5.573    \u001b[0m | \u001b[95m118.3    \u001b[0m |\n",
            "| \u001b[95m8        \u001b[0m | \u001b[95m0.8968   \u001b[0m | \u001b[95m29.3     \u001b[0m | \u001b[95m0.2839   \u001b[0m | \u001b[95m0.9257   \u001b[0m | \u001b[95m9.419    \u001b[0m | \u001b[95m4.493    \u001b[0m | \u001b[95m118.0    \u001b[0m |\n",
            "| \u001b[0m9        \u001b[0m | \u001b[0m0.8957   \u001b[0m | \u001b[0m28.34    \u001b[0m | \u001b[0m0.446    \u001b[0m | \u001b[0m0.3279   \u001b[0m | \u001b[0m10.05    \u001b[0m | \u001b[0m7.185    \u001b[0m | \u001b[0m122.8    \u001b[0m |\n",
            "| \u001b[95m10       \u001b[0m | \u001b[95m0.8996   \u001b[0m | \u001b[95m28.64    \u001b[0m | \u001b[95m0.2249   \u001b[0m | \u001b[95m0.5189   \u001b[0m | \u001b[95m10.35    \u001b[0m | \u001b[95m10.98    \u001b[0m | \u001b[95m117.0    \u001b[0m |\n",
            "| \u001b[0m11       \u001b[0m | \u001b[0m0.8989   \u001b[0m | \u001b[0m29.24    \u001b[0m | \u001b[0m0.305    \u001b[0m | \u001b[0m0.591    \u001b[0m | \u001b[0m10.77    \u001b[0m | \u001b[0m10.25    \u001b[0m | \u001b[0m106.8    \u001b[0m |\n",
            "| \u001b[0m12       \u001b[0m | \u001b[0m0.8948   \u001b[0m | \u001b[0m29.98    \u001b[0m | \u001b[0m0.296    \u001b[0m | \u001b[0m0.7038   \u001b[0m | \u001b[0m10.95    \u001b[0m | \u001b[0m6.979    \u001b[0m | \u001b[0m98.05    \u001b[0m |\n",
            "| \u001b[0m13       \u001b[0m | \u001b[0m0.895    \u001b[0m | \u001b[0m30.0     \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m0.263    \u001b[0m | \u001b[0m11.0     \u001b[0m | \u001b[0m2.083    \u001b[0m | \u001b[0m107.1    \u001b[0m |\n",
            "| \u001b[0m14       \u001b[0m | \u001b[0m0.8778   \u001b[0m | \u001b[0m30.0     \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m2.76     \u001b[0m | \u001b[0m11.0     \u001b[0m | \u001b[0m102.3    \u001b[0m |\n",
            "| \u001b[0m15       \u001b[0m | \u001b[0m0.8955   \u001b[0m | \u001b[0m26.49    \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m1.0      \u001b[0m | \u001b[0m11.0     \u001b[0m | \u001b[0m8.252    \u001b[0m | \u001b[0m112.0    \u001b[0m |\n",
            "=================================================================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1000/1000 [01:00<00:00, 16.49it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running target 2, thresh 6\n",
            "0.898\n",
            "(0.842-0.954)\n",
            "0.492\n",
            "(0.329-0.656)\n",
            "{'0228RF_target_2_6h_4h': ['0.898', '(0.842-0.954)', '0.492', '(0.329-0.656)', 5297]}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hg5di-_t_3Ya"
      },
      "outputs": [],
      "source": [
        "task_map = {0: 'hosp_mort', 1: 'ARF', 2: 'shock'}\n",
        "import os\n",
        "import pandas as pd\n",
        "total_stats = pd.read_hdf(os.path.join('/content/drive/My Drive/ColabNotebooks/MIMIC/Extract', 'MEEP_stats_0702.h5'), \n",
        "                          key = 'total_filling_after_removal')\n",
        "total_stats_1 = pd.read_hdf(os.path.join('/content/drive/My Drive/ColabNotebooks/MIMIC/Extract', 'MEEP_stats_0702.h5'), \n",
        "                       key = 'mimic_intvention_filling')\n",
        "index = np.concatenate((total_stats.index,total_stats_1['col_name']))\n",
        "_DEFAULT_OBJECTIVE = \"minoptsym\"\n",
        "def plot_mean_roc_2(rocs, rocs_1, legend, workname = 'MIMIC_CNN_hosp_mort', auto_flip=True, show_all=False, ax=None, **kwargs):\n",
        "    \"\"\"\n",
        "    Compute and plot the mean ROC curve for a sequence of ROC containers.\n",
        "    rocs:       List of ROC containers created by compute_roc().\n",
        "    auto_flip:  See compute_roc(), applies only to mean ROC curve.\n",
        "    show_all:   If True, show the single ROC curves.\n",
        "                If an integer, show the rocs[:show_all] roc curves.\n",
        "    show_ci:    Show confidence interval\n",
        "    show_ti:    Show tolerance interval\n",
        "    kwargs:     Forwarded to plot_roc(), applies only to mean ROC curve.\n",
        "    Optional kwargs argument show_opt can be either False, True or a string\n",
        "    specifying the particular objective function to be used to plot the\n",
        "    optimal point. See get_objective() for details. Default choice is the\n",
        "    \"minopt\" objective.\n",
        "    \"\"\"\n",
        "    import matplotlib.pyplot as plt\n",
        "    if ax is None:\n",
        "        ax = plt.gca()\n",
        "\n",
        "    n_samples = len(rocs)\n",
        "\n",
        "    # Some default values.\n",
        "    zorder = kwargs.get(\"zorder\", 1)\n",
        "    label = kwargs.pop(\"label\", \"eICU test set\")\n",
        "    label_1 = kwargs.pop(\"label\", \"MIMIC Whole\")\n",
        "    # kwargs for plot_roc()...\n",
        "    show_details = kwargs.get(\"show_details\", False)\n",
        "    show_opt = kwargs.pop(\"show_opt\", False)\n",
        "    show_ti = kwargs.pop(\"show_ti\", True)\n",
        "    show_ci = kwargs.pop(\"show_ci\", True)\n",
        "    color = kwargs.pop(\"color\", \"darkorange\")\n",
        "    color_1 = kwargs.pop(\"color\", \"royalblue\")\n",
        "    is_opt_str = isinstance(show_opt, (str, list, tuple))\n",
        "    # Defaults for mean-ROC.\n",
        "    resolution = kwargs.pop(\"resolution\", 101)\n",
        "    objective = show_opt if is_opt_str else _DEFAULT_OBJECTIVE\n",
        "\n",
        "    # Compute average ROC.\n",
        "    ret_mean = compute_mean_roc(rocs=rocs,\n",
        "                                resolution=resolution,\n",
        "                                auto_flip=auto_flip,\n",
        "                                objective=objective)\n",
        "    ret_mean_1 = compute_mean_roc(rocs=rocs_1,\n",
        "                                resolution=resolution,\n",
        "                                auto_flip=auto_flip,\n",
        "                                objective=objective)\n",
        "\n",
        "    # Plot ROC curve for single bootstrap samples.\n",
        "    if show_all:\n",
        "        def isint(x):\n",
        "            return isinstance(x, int) and not isinstance(x, bool)\n",
        "        n_loops = show_all if isint(show_all) else np.inf\n",
        "        n_loops = min(n_loops, len(rocs))\n",
        "        for ret in rocs[:n_loops]:\n",
        "            ax.plot(ret.fpr, ret.tpr,\n",
        "                    color=\"gray\",\n",
        "                    alpha=0.2,\n",
        "                    zorder=zorder + 2)\n",
        "        for ret in rocs_1[:n_loops]:\n",
        "            ax.plot(ret.fpr, ret.tpr,\n",
        "                    color=\"gray\",\n",
        "                    alpha=0.2,\n",
        "                    zorder=zorder + 2)\n",
        "    if show_ti:\n",
        "        # 95% interval\n",
        "        tpr_sort = np.sort(ret_mean.tpr_all, axis=0)\n",
        "        tpr_lower = tpr_sort[int(0.025 * n_samples), :]\n",
        "        tpr_upper = tpr_sort[int(0.975 * n_samples), :]\n",
        "        label_int = \"95% of all samples\" if show_details else None\n",
        "        ax.fill_between(ret_mean.fpr, tpr_lower, tpr_upper,\n",
        "                        color=\"gray\", alpha=.2,\n",
        "                        label=label_int,\n",
        "                        zorder=zorder + 1)\n",
        "        tpr_sort_1 = np.sort(ret_mean_1.tpr_all, axis=0)\n",
        "        tpr_lower_1 = tpr_sort_1[int(0.025 * n_samples), :]\n",
        "        tpr_upper_1= tpr_sort_1[int(0.975 * n_samples), :]\n",
        "        label_int = \"95% of all samples\" if show_details else None\n",
        "        ax.fill_between(ret_mean_1.fpr, tpr_lower_1, tpr_upper_1,\n",
        "                        color=\"gray\", alpha=.2,\n",
        "                        label=label_int,\n",
        "                        zorder=zorder + 1)\n",
        "    if show_ci:\n",
        "        # 95% confidence interval\n",
        "        tpr_std = np.std(ret_mean.tpr_all, axis=0, ddof=1)\n",
        "        tpr_lower = ret_mean.tpr - 1.96 * tpr_std / np.sqrt(n_samples)\n",
        "        tpr_upper = ret_mean.tpr + 1.96 * tpr_std / np.sqrt(n_samples)\n",
        "        label_ci = \"95% CI of mean curve\" if show_details else None\n",
        "        ax.fill_between(ret_mean.fpr, tpr_lower, tpr_upper,\n",
        "                        color=color, alpha=.3,\n",
        "                        label=label_ci,\n",
        "                        zorder=zorder)\n",
        "        tpr_std_1 = np.std(ret_mean_1.tpr_all, axis=0, ddof=1)\n",
        "        tpr_lower_1 = ret_mean_1.tpr - 1.96 * tpr_std_1 / np.sqrt(n_samples)\n",
        "        tpr_upper_1 = ret_mean_1.tpr + 1.96 * tpr_std_1 / np.sqrt(n_samples)\n",
        "        label_ci = \"95% CI of mean curve\" if show_details else None\n",
        "        ax.fill_between(ret_mean_1.fpr, tpr_lower_1, tpr_upper_1,\n",
        "                        color=color_1, alpha=.3,\n",
        "                        label=label_ci,\n",
        "                        zorder=zorder)\n",
        "\n",
        "    # Last but not least, plot the average ROC curve on top  of everything.\n",
        "    plot_roc(roc=ret_mean, label=label, show_opt=show_opt,\n",
        "             color=color, ax=ax, zorder=zorder + 3, **kwargs)\n",
        "    plot_roc(roc=ret_mean_1, label=label_1, show_opt=show_opt,\n",
        "             color=color_1, ax=ax, zorder=zorder + 3, **kwargs)\n",
        "    plt.legend(title=legend)\n",
        "    plt.savefig('./checkpoints/' + workname + '_auc.eps',format='eps', bbox_inches = 'tight', pad_inches = 0.1, dpi=1200)\n",
        "    return ret_mean"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cEZwxZt5cM0u"
      },
      "outputs": [],
      "source": [
        "a = st.t.interval(alpha=0.95, df=len(roc), loc=np.mean(roc), scale=np.std(roc))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gN3TcF4OmNBV"
      },
      "outputs": [],
      "source": [
        "\n",
        "# ctype, count= np.unique(trainval_target, return_counts=True)\n",
        "# total_trainval_samples = len(trainval_target)\n",
        "# binary_weight = np.asarray([ total_trainval_samples / k / len(ctype) for k in count])\n",
        "# weight_dict = {}\n",
        "# for i, c in enumerate(ctype):\n",
        "#     weight_dict[c] = total_trainval_samples / count[i] / len(ctype)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CgzQDVM80hry"
      },
      "outputs": [],
      "source": [
        "result_dict = {empty_list: [] for empty_list in range(5)}\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qtdtQeXWhTCV",
        "outputId": "3f0ab4e7-145d-4221-e7ef-9c6d7247e776"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{0: ['0.709', '(0.540-0.878)'], 1: [], 2: [], 3: [], 4: []}"
            ]
          },
          "execution_count": 70,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "result_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SKK3XGYsgRFk"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "twzjgxsEgSOv"
      },
      "outputs": [],
      "source": [
        "f.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oKHbXbnngmzG"
      },
      "outputs": [],
      "source": [
        "f = open(\"./MIMIC_RF.pkl\",'rb')\n",
        "object_file = pickle.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dQHwkJq-jGWP",
        "outputId": "9a487096-a1c8-47d1-ff41-6c0170924494"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{0: ['0.709', '(0.540-0.878)'], 1: [], 2: [], 3: [], 4: []}"
            ]
          },
          "execution_count": 78,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "object_file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s4lcmq6TdPY4",
        "outputId": "79b5d7be-e854-4694-97de-772095a088f8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['0.709', '(0.540-0.878)']"
            ]
          },
          "execution_count": 62,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "result_dict[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UHVXWsXuTts1",
        "outputId": "992fac26-3f51-4ae4-c384-51369f6c8e6a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.12544361530068304"
            ]
          },
          "execution_count": 56,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "p_dict = xgb_bo.max['params']\n",
        "p_dict['C']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iyCWlaDysf14",
        "outputId": "8f5e9404-cf09-4448-ac6e-1a3233b0f380"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "max_iter reached after 36 seconds\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:354: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  ConvergenceWarning,\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   35.7s finished\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "LogisticRegression(C=0.1254, class_weight='balanced', l1_ratio=1.0,\n",
              "                   penalty='elasticnet', random_state=0, solver='saga',\n",
              "                   verbose=1)"
            ]
          },
          "execution_count": 47,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# eval on itself and the other dataset \n",
        "regressor = LogisticRegression(penalty='elasticnet', dual=False,  C=0.1254, fit_intercept=True, intercept_scaling=1, \\\n",
        "                                class_weight= 'balanced', random_state=0, solver='saga', max_iter= 100, \\\n",
        "                                verbose=1, warm_start=False, n_jobs=None, l1_ratio=1.0)\n",
        "\n",
        "regressor.fit(trainval_data, trainval_target)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JVJITVEQxTpA",
        "outputId": "50e37f19-22a8-4a20-a1c0-c86eb03e8cd3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1000/1000 [00:20<00:00, 48.43it/s]\n"
          ]
        }
      ],
      "source": [
        "## bootstrapping on its own testset \n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import average_precision_score\n",
        "roc = []\n",
        "prc = []\n",
        "for i in tqdm(range(1000)):\n",
        "    test_index = np.random.choice(len(test_target), 1000)\n",
        "    test_i = [test_head[i] for i in test_index]\n",
        "    test_t = test_target[test_index]\n",
        "    test_data = np.stack(test_i).reshape((len(test_t), -1))\n",
        "    test_pred_prob= regressor.predict_proba(test_data)\n",
        "    test_roc = roc_auc_score(test_t, test_pred_prob[:, 1])\n",
        "    test_prc = average_precision_score(test_t, test_pred_prob[:, 1])\n",
        "    roc.append(test_roc)\n",
        "    prc.append(test_prc)\n",
        "#create 95% confidence interval for population mean weight\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TZ_guadZIt12",
        "outputId": "a06974ef-bead-4d14-fa68-81e4d2ad20ee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.846\n",
            "(0.810-0.882)\n",
            "0.613\n",
            "(0.532-0.695)\n"
          ]
        }
      ],
      "source": [
        "import scipy.stats as st\n",
        "#create 95% confidence interval for population mean weight\n",
        "print('%.3f'%np.mean(roc))\n",
        "print('(%.3f-%.3f)'%st.t.interval(alpha=0.95, df=len(roc), loc=np.mean(roc), scale=np.std(roc)))\n",
        "print('%.3f'%np.mean(prc))\n",
        "print('(%.3f-%.3f)'%st.t.interval(alpha=0.95, df=len(prc), loc=np.mean(prc), scale=np.std(prc)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qZYlmNDsKIpf"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2RPXL_7B56ca"
      },
      "outputs": [],
      "source": [
        "# load the other \n",
        "# eicu test loader \n",
        "# load EICU DATA \n",
        "import importlib\n",
        "from tqdm import tqdm\n",
        "import scipy.stats as st\n",
        "importlib.reload(prepare_data)\n",
        "from sklearn.metrics import average_precision_score\n",
        "\n",
        "data_label = np.load('/content/drive/My Drive/Colab Notebooks/MIMIC/Extract/MEEP/eICU_compile_0705_2022_1.npy', \\\n",
        "                allow_pickle=True).item()\n",
        "\n",
        "etrain_head = data_label['train_head']\n",
        "estatic_train_filter = data_label['static_train_filter']\n",
        "# train_sofa_tail = data_label['train_sofa_tail']\n",
        "# train_sofa_head = data_label['train_sofa_head']\n",
        "edev_head = data_label['dev_head']\n",
        "estatic_dev_filter = data_label['static_dev_filter']\n",
        "# dev_sofa_tail = data_label['dev_sofa_tail']\n",
        "# dev_sofa_head =['dev_sofa_head']\n",
        "etest_head = data_label['test_head']\n",
        "estatic_test_filter = data_label['static_test_filter']\n",
        "# test_sofa_tail = data_label['test_sofa_tail']\n",
        "# test_sofa_head =['test_sofa_head']\n",
        "es_train = np.stack(estatic_train_filter, axis=0)\n",
        "es_dev = np.stack(estatic_dev_filter, axis=0)\n",
        "es_test = np.stack(estatic_test_filter, axis=0) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q-AJNHO0KXg1"
      },
      "outputs": [],
      "source": [
        "if args.target_index == 0 and args.filter_los == True:\n",
        "\n",
        "    print('Before filtering, train size is %d'%(len(etrain_head)))\n",
        "    es_train, etrain_data = filter_los(es_train, etrain_head, args.thresh, args.gap)\n",
        "    es_dev, edev_data = filter_los(es_dev, edev_head, args.thresh, args.gap)\n",
        "    es_test, etest_data = filter_los(es_test, etest_head, args.thresh, args.gap)\n",
        "    print('After filtering, train size is %d'%(len(etrain_head)))\n",
        "    etrain_label = es_train[:, 0]\n",
        "    edev_label= es_dev[:, 0]\n",
        "    etest_label = es_test[:, 0]\n",
        "\n",
        "elif args.target_index == 1:\n",
        "    etrain_data, etrain_label = filter_arf(args, etrain_head)\n",
        "    edev_data, edev_label = filter_arf(args, edev_head)\n",
        "    etest_data, etest_label = filter_arf(args, etest_head)\n",
        "\n",
        "elif args.target_index == 2:\n",
        "    etrain_data, etrain_label = filter_shock(args, etrain_head)\n",
        "    edev_data, edev_label = filter_shock(args, edev_head)\n",
        "    etest_data, etest_label = filter_shock(args, etest_head)\n",
        "\n",
        "crossval_target = np.concatenate((etrain_label, edev_label, etest_label), axis= 0)\n",
        "crossval_head = np.stack(etrain_data + edev_data + etest_data, axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pk_08KLW6ezO",
        "outputId": "b0c95a79-ec8b-4090-e86d-92627ca07ac6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1000/1000 [00:09<00:00, 103.80it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.709\n",
            "(0.540-0.878)\n",
            "0.264\n",
            "(0.258-0.271)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# bootstrapping\n",
        "## bootstrapping on its own testset \n",
        "for i in tqdm(range(1000)):\n",
        "    test_index = np.random.choice(len(crossval_target), 1000)\n",
        "    test_i = [crossval_head[i] for i in test_index]\n",
        "    test_t = crossval_target[test_index].astype(int)\n",
        "    test_data = np.stack(test_i).reshape((len(test_t), -1))\n",
        "    test_pred_prob= regressor.predict_proba(test_data)\n",
        "    test_roc = roc_auc_score(test_t, test_pred_prob[:, 1])\n",
        "    test_prc = average_precision_score(test_t, test_pred_prob[:, 1])\n",
        "    roc.append(test_roc)\n",
        "    prc.append(test_prc)\n",
        "#create 95% confidence interval for population mean weight\n",
        "print('%.3f'%np.mean(roc))\n",
        "print('(%.3f-%.3f)'%st.t.interval(alpha=0.95, df=len(roc), loc=np.mean(roc), scale=np.std(roc)))\n",
        "print('%.3f'%np.mean(prc))\n",
        "print('(%.3f-%.3f)'%st.t.interval(alpha=0.95, df=len(prc), loc=np.mean(prc), scale=np.std(prc)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hK32OkdxhkKI",
        "outputId": "cf585a8c-46be-43d5-a04f-c31f906d218a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.44734120207157296"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "np.mean(prc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lUooM5lW6wLN"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.hist(prc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "piSc_40I6UKw"
      },
      "outputs": [],
      "source": [
        "trainval_target = np.concatenate((train_target, dev_target), axis=0)\n",
        "trainval_data = np.stack(train_head + dev_head, axis=0).reshape((len(trainval_target), -1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Eo63yFv8x1b5"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import roc_auc_score\n",
        "auroc = roc_auc_score(test_target, test_pred[:, 1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L2e8WQ7ayyKw"
      },
      "outputs": [],
      "source": [
        "import sklearn.metrics as metrics\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "num_class = 2\n",
        "cm = metrics.confusion_matrix(test_target, test_pred)\n",
        "label_x = None\n",
        "label_y = None \n",
        "cf_matrix = cm/np.repeat(np.expand_dims(np.sum(cm, axis=1), axis=-1), num_class, axis=1)\n",
        "group_counts = ['{0:0.0f}'.format(value) for value in cm.flatten()]\n",
        "# percentage based on true label \n",
        "gr = (cm/np.repeat(np.expand_dims(np.sum(cm, axis=1), axis=-1), num_class, axis=1)).flatten()\n",
        "group_percentages = ['{0:.2%}'.format(value) for value in gr]\n",
        "\n",
        "labels = [f'{v1}\\n{v2}' for v1, v2 in zip(group_percentages, group_counts)]\n",
        "\n",
        "labels = np.asarray(labels).reshape(num_class, num_class)\n",
        "\n",
        "if label_x is not None:\n",
        "    xlabel = label_x\n",
        "    ylabel = label_y\n",
        "else:\n",
        "    xlabel = ['Pred-%d'%i for i in range(num_class)]\n",
        "    ylabel = ['%d'%i for i in range(num_class)]\n",
        "\n",
        "sns.set(font_scale = 1.5)\n",
        "\n",
        "hm = sns.heatmap(cf_matrix, annot=labels, fmt='', cmap = 'OrRd', \\\n",
        "annot_kws={\"fontsize\": 16}, xticklabels=xlabel, yticklabels=ylabel, cbar=False)\n",
        "# hm.set(title=title)\n",
        "fig = plt.gcf()\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}